# -*- coding: utf-8 -*-
"""YoLo V8 - People Detection and Tracking in ROI

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/vitthalsawant/yolo-v8-people-detection-and-tracking-in-roi.e5578211-ea1b-42f6-891c-ecc6d0114cf6.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251208/auto/storage/goog4_request%26X-Goog-Date%3D20251208T180517Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D85eda13bf50d0af0c7ee227a27f97219b6ce2769f0a24b53b1544ed5fc02da8a276f36f49b2b42f523864367e6542020cab714aeb4ff32583cbf01a9b0a9f4004402201c79ca19f20b6909696597f53a8212c5912ceb3c67659184516fbe51dc72a00ecdaf287e7c3b4aeb313395397c740131fd9480fed6675501891209acc29d1e8cc549e9240db8b18605ec9f052957fb93e9645ea0cdc1c95704c586882f5c8817c376803e2d47b4860ee197fc08fe636dff02d80420d6f95035e9c83309f21522f6791cf46f5c97de6708b6630625603a25866d882c451e3408554763ad44df2170d9a8a3ff6891fb026d8f3e1cb1c4a772e11a930457ac08e50e83f0c0
"""

# Object Detection
import cv2
from ultralytics import YOLO

# Basics
import pandas as pd
import numpy as np
import time
import os
import subprocess
import platform
from datetime import datetime

"""# A People Detection and Counting project in a ROI based on the Yolo V8 Model.
----------------------
 **The objectives of the project are:**
- To detect people that are passing in a Region of interest (ROI)
- Track each individual with a unique ID in the ROI
- Live camera feed with real-time display

---------------------
"""

#loading a YOLO model
model = YOLO('yolov8x.pt')

#geting names from classes
dict_classes = model.model.names

# Auxiliary functions
def risize_frame(frame, scale_percent):
    """Function to resize an image in a percent scale"""
    width = int(frame.shape[1] * scale_percent / 100)
    height = int(frame.shape[0] * scale_percent / 100)
    dim = (width, height)

    # resize image
    resized = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)
    return resized



def filter_tracks(centers, patience):
    """Function to filter track history"""
    filter_dict = {}
    for k, i in centers.items():
        d_frames = i.items()
        filter_dict[k] = dict(list(d_frames)[-patience:])

    return filter_dict


def update_tracking(centers_old,obj_center, thr_centers, lastKey, frame, frame_max):
    """Function to update track of objects"""
    is_new = 0
    lastpos = [(k, list(center.keys())[-1], list(center.values())[-1]) for k, center in centers_old.items()]
    lastpos = [(i[0], i[2]) for i in lastpos if abs(i[1] - frame) <= frame_max]
    # Calculating distance from existing centers points
    previous_pos = [(k,obj_center) for k,centers in lastpos if (np.linalg.norm(np.array(centers) - np.array(obj_center)) < thr_centers)]
    # if distance less than a threshold, it will update its positions
    if previous_pos:
        id_obj = previous_pos[0][0]
        centers_old[id_obj][frame] = obj_center

    # Else a new ID will be set to the given object
    else:
        if lastKey:
            last = lastKey.split('D')[1]
            id_obj = 'ID' + str(int(last)+1)
        else:
            id_obj = 'ID0'

        is_new = 1
        centers_old[id_obj] = {frame:obj_center}
        lastKey = list(centers_old.keys())[-1]


    return centers_old, id_obj, is_new, lastKey

"""# Detecting People in ROI"""

### Configurations
#Verbose during prediction
verbose = False
# Scaling percentage of original frame
scale_percent = 100
# model confidence level
conf_level = 0.8
# Threshold of centers ( old\new)
thr_centers = 20
#Number of max frames to consider a object lost
frame_max = 5
# Number of max tracked centers stored
patience = 100
# ROI area color transparency
alpha = 0.1
# Duration to run in seconds
duration_seconds = 30
# Camera index (0 for default camera)
camera_index = 0
#-------------------------------------------------------
# Initialize camera
print('[INFO] - Initializing camera...')
video = cv2.VideoCapture(camera_index)

if not video.isOpened():
    print('[ERROR] - Could not open camera. Please check if camera is connected.')
    exit(1)

# Objects to detect Yolo
class_IDS = [0]
# Auxiliary variables
centers_old = {}

obj_id = 0
end = []
frames_list = []
count_p = 0
lastKey = ''
print(f'[INFO] - Verbose during Prediction: {verbose}')


# Set camera resolution (optional, adjust as needed)
video.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
video.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

# Get camera properties
width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = video.get(cv2.CAP_PROP_FPS) or 30  # Default to 30 if not available

print('[INFO] - Camera Resolution: ', (width, height))
print('[INFO] - Camera FPS: ', fps)

# Scaling Video for better performance
if scale_percent != 100:
    print('[INFO] - Scaling change may cause errors in pixels lines ')
    width = int(width * scale_percent / 100)
    height = int(height * scale_percent / 100)
    print('[INFO] - Dim Scaled: ', (width, height))

# ROI coordinates - adjust based on your camera view
# Default: center region of the frame
roi_x_start = int(width * 0.2)  # 20% from left
roi_x_end = int(width * 0.8)    # 80% from left
roi_y_start = int(height * 0.2) # 20% from top
roi_y_end = int(height * 0.8)   # 80% from top

print(f'[INFO] - ROI coordinates: ({roi_x_start}, {roi_y_start}) to ({roi_x_end}, {roi_y_end})')
print(f'[INFO] - Running for {duration_seconds} seconds...')
print('[INFO] - Press "q" to quit early')

#-------------------------------------------------------
### Video output setup ####
# Create output filename with timestamp
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
script_dir = os.path.dirname(os.path.abspath(__file__))
output_filename = f'people_detection_{timestamp}.mp4'
output_path = os.path.join(script_dir, output_filename)

# Video codec and writer
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
output_video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

print(f'[INFO] - Video will be saved to: {output_path}')


#-------------------------------------------------------
# Executing Recognition
start_time = time.time()
frame_count = 0

print('[INFO] - Starting detection and tracking...')

while True:
    # Check if 30 seconds have passed
    elapsed_time = time.time() - start_time
    remaining_time = duration_seconds - elapsed_time
    
    if elapsed_time >= duration_seconds:
        print(f'\n[INFO] - {duration_seconds} seconds completed. Stopping...')
        break
    
    # reading frame from camera
    ret, frame = video.read()
    
    if not ret:
        print('[WARNING] - Failed to grab frame')
        break
    
    #Applying resizing of read frame
    frame = risize_frame(frame, scale_percent)
    
    # Update ROI coordinates based on current frame size
    current_height, current_width = frame.shape[:2]
    roi_x_start = int(current_width * 0.2)
    roi_x_end = int(current_width * 0.8)
    roi_y_start = int(current_height * 0.2)
    roi_y_end = int(current_height * 0.8)
    
    # Define ROI polygon (rectangle shape)
    area_roi = [np.array([
        (roi_x_start, roi_y_start),
        (roi_x_end, roi_y_start),
        (roi_x_end, roi_y_end),
        (roi_x_start, roi_y_end)
    ], np.int32)]
    
    # Extract ROI region
    ROI = frame[roi_y_start:roi_y_end, roi_x_start:roi_x_end]


    if verbose:
        print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))

    # Getting predictions
    y_hat = model.predict(ROI, conf = conf_level, classes = class_IDS, device = 'cpu', verbose = False)

    # Getting the bounding boxes, confidence and classes of the recognize objects in the current frame.
    boxes   = y_hat[0].boxes.xyxy.cpu().numpy()
    conf    = y_hat[0].boxes.conf.cpu().numpy()
    classes = y_hat[0].boxes.cls.cpu().numpy()

    # Storing the above information in a dataframe
    positions_frame = pd.DataFrame(np.hstack((boxes, conf[:, None], classes[:, None])), columns = ['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])

    #Translating the numeric class labels to text
    labels = [dict_classes[i] for i in classes]


    # For each people, draw the bounding-box and counting each one the pass thought the ROI area
    for ix, row in enumerate(positions_frame.iterrows()):
        # Getting the coordinates of each vehicle (row)
        xmin, ymin, xmax, ymax, confidence, category,  = row[1].astype('int')

        # Calculating the center of the bounding-box
        center_x, center_y = int(((xmax+xmin))/2), int((ymax+ ymin)/2)

        # Adjust center coordinates to full frame coordinates
        center_x_full = center_x + roi_x_start
        center_y_full = center_y + roi_y_start
        
        #Updating the tracking for each object
        centers_old, id_obj, is_new, lastKey = update_tracking(centers_old, (center_x_full, center_y_full), thr_centers, lastKey, frame_count, frame_max)


        #Updating people in roi
        count_p+=is_new

        # drawing center and bounding-box in the ROI
        cv2.rectangle(ROI, (xmin, ymin), (xmax, ymax), (0,0,255), 2) # box
        
        # Draw tracking centers on ROI (convert from full frame to ROI coordinates)
        for center_x_full, center_y_full in centers_old[id_obj].values():
            center_x_roi = center_x_full - roi_x_start
            center_y_roi = center_y_full - roi_y_start
            # Only draw if within ROI bounds
            if 0 <= center_x_roi < (roi_x_end - roi_x_start) and 0 <= center_y_roi < (roi_y_end - roi_y_start):
                cv2.circle(ROI, (center_x_roi, center_y_roi), 5, (0,0,255), -1) # center of box

        #Drawing above the bounding-box the name of class recognized.
        cv2.putText(img=ROI, text=id_obj+':'+str(np.round(conf[ix],2)),
                    org= (xmin,ymin-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=0.8, color=(0, 0, 255),thickness=1)



    # Drawing the number of people and timer
    cv2.putText(img=frame, text=f'People Count: {count_p}',
                org=(30, 40), fontFace=cv2.FONT_HERSHEY_TRIPLEX,
                fontScale=1.2, color=(0, 255, 0), thickness=2)
    
    # Display remaining time
    time_text = f'Time: {int(remaining_time)}s'
    cv2.putText(img=frame, text=time_text,
                org=(30, 80), fontFace=cv2.FONT_HERSHEY_TRIPLEX,
                fontScale=1.0, color=(0, 255, 255), thickness=2)
    
    # Display FPS
    if frame_count > 0:
        current_fps = frame_count / elapsed_time
        fps_text = f'FPS: {current_fps:.1f}'
        cv2.putText(img=frame, text=fps_text,
                    org=(30, 120), fontFace=cv2.FONT_HERSHEY_TRIPLEX,
                    fontScale=0.8, color=(255, 255, 0), thickness=2)

    # Filtering tracks history
    centers_old = filter_tracks(centers_old, patience)
    if verbose:
        print(f'People count: {count_p}, Time: {elapsed_time:.1f}s')

    # Drawing the ROI area
    overlay = frame.copy()
    cv2.polylines(overlay, pts=area_roi, isClosed=True, color=(255, 0, 0), thickness=2)
    cv2.fillPoly(overlay, area_roi, (255, 0, 0))
    frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)
    
    # Draw tracking centers on full frame (for visualization)
    for id_obj, centers_dict in centers_old.items():
        for center_x_full, center_y_full in centers_dict.values():
            # Only draw if within frame bounds
            if 0 <= center_x_full < current_width and 0 <= center_y_full < current_height:
                cv2.circle(frame, (center_x_full, center_y_full), 5, (0, 0, 255), -1)

    # Display the frame
    cv2.imshow('People Detection and Tracking in ROI', frame)
    
    # Write frame to video file
    output_video.write(frame)
    
    # Increment frame count
    frame_count += 1
    
    # Break if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        print('\n[INFO] - Quit key pressed. Stopping...')
        break

# Releasing the camera and video writer
video.release()
output_video.release()
cv2.destroyAllWindows()


# Final summary
total_time = time.time() - start_time
print('\n' + '='*50)
print('[INFO] - Detection completed!')
print(f'[INFO] - Total time: {total_time:.2f} seconds')
print(f'[INFO] - Total frames processed: {frame_count}')
print(f'[INFO] - Average FPS: {frame_count/total_time:.2f}')
print(f'[INFO] - Total people detected in ROI: {count_p}')
print('='*50)

# Video file information
if os.path.exists(output_path):
    file_size = os.path.getsize(output_path) / (1024 * 1024)  # Size in MB
    print(f'\n[INFO] - Video saved successfully!')
    print(f'[INFO] - File: {output_path}')
    print(f'[INFO] - File size: {file_size:.2f} MB')
    print(f'\n[INFO] - You can find the video at:')
    print(f'        {output_path}')
    
    # Try to open the video file
    try:
        if platform.system() == 'Windows':
            os.startfile(output_path)
            print(f'\n[INFO] - Video opened in default player!')
        elif platform.system() == 'Darwin':  # macOS
            subprocess.run(['open', output_path])
            print(f'\n[INFO] - Video opened in default player!')
        else:  # Linux
            subprocess.run(['xdg-open', output_path])
            print(f'\n[INFO] - Video opened in default player!')
    except Exception as e:
        print(f'\n[INFO] - Could not auto-open video. Please open manually: {output_path}')
else:
    print(f'\n[WARNING] - Video file not found at: {output_path}')

