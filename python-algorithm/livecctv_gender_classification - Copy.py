# -*- coding: utf-8 -*-
"""CCTV Gender Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/vitthalsawant/cctv-gender-classification.8433a6fe-f130-4f01-a86b-5c6c13600e8a.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251213/auto/storage/goog4_request%26X-Goog-Date%3D20251213T182925Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6b373e34eca91d34a7af3efcd05ee26c0c4185cf92c55f7c58e1400a6573703f6a54218ba8936ec74089fab046567803914ef2aee95a43f6b5d72e54989c1cd120a7204c827e50250bba771e0e7a278b185cfdf9700b737b18a27613af4e7331c54e35f8d1208ccbf1eb7022526ea0dcc5ed264ebc1f8673953aac4f9a75a07812f29c0243ca15d90e3d5f318d2b624a4e4657239f97e552705c63d299be5bc2866ffd9d4e76cf74202a272eec853b8ce9bfc66f4184b957a01dc017166e2fd73aa6e73edc57c4023407a69c3783e4b103a7c4ecfc2ff4985ae283ba2067f2f31d3ba64b7a175a1a792769779b9b7a209a1c1c27938eee9237d12a746f3c2546
"""

# Dataset paths updated to use local Windows paths
print('Data source import complete.')

import os
import random
import shutil
import glob as gb
from tqdm import tqdm
from collections import Counter
import cv2
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
from PIL import Image, ImageTk
import threading

import numpy as np
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend to avoid tkinter conflicts
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns

from tensorflow.keras.preprocessing.image import load_img,img_to_array
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, AveragePooling2D,Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.models import Model, load_model

from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix

def visualize_random_images(male_folder, female_folder, num_images=5):
    male_images = os.listdir(male_folder)
    female_images = os.listdir(female_folder)

    fig, axes = plt.subplots(2, num_images, figsize=(15, 7))
    plt.subplots_adjust(wspace=0.3, hspace=0.3)

    for i in range(num_images):
        male_image_path = os.path.join(male_folder, random.choice(male_images))
        female_image_path = os.path.join(female_folder, random.choice(female_images))

        male_img = plt.imread(male_image_path)
        female_img = plt.imread(female_image_path)

        axes[0, i].imshow(male_img)
        axes[0, i].axis('off')
        axes[0, i].set_title('Male')

        axes[1, i].imshow(female_img)
        axes[1, i].axis('off')
        axes[1, i].set_title('Female')

    plt.savefig('sample_images.png')
    plt.close()

# Updated paths to use local Windows directories
male_folder = r'C:\Users\vitth\Downloads\CCTV Gender Classifier Dataset\CCTV Gender Classifier Dataset\MALE'
female_folder = r'C:\Users\vitth\Downloads\CCTV Gender Classifier Dataset\CCTV Gender Classifier Dataset\FEMALE'

visualize_random_images(male_folder, female_folder)

num_male_images = len(os.listdir(male_folder))
num_female_images = len(os.listdir(female_folder))

print(f"Total number of male images: {num_male_images}")
print(f"Total number of female images: {num_female_images}")

# Create bar plot
categories = ['Male', 'Female']
num_images = [num_male_images, num_female_images]

plt.bar(categories, num_images, color=['blue', 'pink'])
plt.xlabel('Category')
plt.ylabel('Number of Images')
plt.title('Number of Images in Each Category')
plt.savefig('dataset_distribution.png')
plt.close()  # Close instead of show to avoid tkinter conflicts

DataPath = r'C:\Users\vitth\Downloads\CCTV Gender Classifier Dataset\CCTV Gender Classifier Dataset'

labels2int={"MALE":0,"FEMALE":1}
int2labels={0:"MALE",1:"FEMALE"}

Data = []
Classes = []

# Use only a small subset of data for faster training (20% of available data)
USE_SMALL_DATASET = True
SUBSET_PERCENTAGE = 0.2  # Use only 20% of data

# Iterate through each folder in DataPath
for folder in os.listdir(DataPath):
    print(folder)

    # Construct the full path to the folder
    folder_path = os.path.join(DataPath, folder)

    # Get a list of all files in the folder
    files = os.listdir(folder_path)
    
    # Limit the number of files if using small dataset
    if USE_SMALL_DATASET:
        num_files_to_use = max(1, int(len(files) * SUBSET_PERCENTAGE))
        files = random.sample(files, min(num_files_to_use, len(files)))
        print(f"Using {len(files)} out of {len(os.listdir(folder_path))} files from {folder} (small dataset mode)")

    print(f"Data found {len(files)} in {folder}")

    # Iterate through each file in the folder
    for file in files:
        # Construct the full path to the file
        file_path = os.path.join(folder_path, file)

        # Load the image, resize it, and convert it to an array
        img = load_img(file_path, target_size=(200, 100))
        img_array = img_to_array(img)

        # Preprocess the image array
        img_array = preprocess_input(img_array)

        # Append the preprocessed image array to the Data list
        Data.append(img_array)

        # Append the label to the Classes list based on the folder name
        Classes.append(labels2int[folder])

print("Length of Classes before binarization:", len(Classes))

lb = LabelBinarizer()
Classes = lb.fit_transform(Classes)
Classes = to_categorical(Classes)

print("Shape of Classes after binarization and conversion:", Classes.shape)

Data = np.array(Data,dtype = "float32")
Classes = np.array(Classes)
# Split data into training and testing sets (80% train, 20% test)
Data_train, Data_test, Classes_train, Classes_test = train_test_split(Data, Classes, test_size=0.2, stratify=Classes, random_state=42, shuffle=True)

# Split training data into training and validation sets (80% train, 20% validation)
Data_train, Data_val, Classes_train, Classes_val = train_test_split(Data_train, Classes_train, test_size=0.2, stratify=Classes_train, random_state=42)

# Free up memory by deleting the original Data array
del Data

print("Shape of training data (Data_train):", Data_train.shape)
print("Shape of Validation data (Data_val):", Data_val.shape)
print("Shape of testing data (Data_test):", Data_test.shape)
print('=============================================================')
print("Shape of training labels (Classes_train):", Classes_train.shape)
print("Shape of Validation labels (Classes_val):", Classes_val.shape)
print("Shape of testing labels (Classes_test):", Classes_test.shape)

def resnet50_modelarch():
    lr = 1e-5
    epochs = 10

    basemodel= ResNet50(include_top=False, input_shape=(200,100,3))
    headmodel= basemodel.output
    headmodel= AveragePooling2D(pool_size=(3,3))(headmodel)

    #headmodel = Dense(1024, activation="relu")(headmodel)
    #headmodel = Dropout(0.3)(headmodel)
    headmodel = Flatten(name="flatten")(headmodel)
    headmodel = Dense(512, activation="relu")(headmodel)
    headmodel = Dropout(0.3)(headmodel)
    headmodel = Dense(256, activation="relu")(headmodel)
    headmodel = Dropout(0.3)(headmodel)
    headmodel = Dense(128, activation="relu")(headmodel)
    headmodel = Dropout(0.3)(headmodel)
    headmodel = Dense(64, activation="relu")(headmodel)
    #headmodel = Dropout(0.3)(headmodel)
    headmodel = Dense(2, activation='softmax')(headmodel)

    model=Model(inputs=basemodel.input, outputs=headmodel)

    for layer in basemodel.layers:
        layer.trainable=False

    opt=Adam(learning_rate=lr, decay=lr / epochs)
    model.compile(loss="binary_crossentropy",optimizer=opt,metrics=["accuracy"])
    callback = EarlyStopping(monitor='val_loss',patience=6)

    return model,callback,epochs

model,callback,Epochs = resnet50_modelarch()
model.summary()

history = model.fit(Data_train,Classes_train,
                    batch_size=32,
                    validation_data=(Data_val,Classes_val),
                    epochs=10,callbacks=[callback])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('acc')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validate'], loc='upper left')
plt.savefig('model_accuracy.png')
plt.close()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validate'], loc='upper left')
plt.savefig('model_loss.png')
plt.close()

scores = model.evaluate(Data_test,Classes_test, verbose=1)
print("ResNet50 Score:",scores[1])

predIdxs = model.predict(Data_test,batch_size=32)
predIdxs=np.argmax(predIdxs,axis=1)

from sklearn.metrics import classification_report,confusion_matrix
testy_res = Classes_test.argmax(axis=1)
CR = classification_report(testy_res, predIdxs)
print(CR)

# Get predictions for the test set
predictions = model.predict(Data_test)

# Convert predictions to class labels (0 for male, 1 for female)
predicted_labels = np.argmax(predictions, axis=1)

# Compare predicted labels with true labels
correct_predictions = np.sum(predicted_labels == np.argmax(Classes_test, axis=1))
total_samples = len(Classes_test)

# Calculate accuracy
accuracy = correct_predictions / total_samples
print("Test Accuracy:", accuracy)

class_names=['MALE','FEMALE']

import random
def plot_prediction(Data_test, Classes_test, n_images, class_names):
    """
    Test the model on random predictions and plot the results.

    Args:
        Data_test (numpy array): The test images data.
        Classes_test (numpy array): The true labels for the test images.
        n_images (int): Number of images to plot.
        class_names (list): List of class names.
    """
    # Get the total number of test images
    num_images = len(Data_test)

    # Randomly select indices for the images to plot
    random_indices = random.sample(range(num_images), n_images)

    # Make predictions on the selected test data
    predictions = np.argmax(model.predict(Data_test[random_indices]), axis=1)

    # Plot the randomly selected images along with their true labels and predictions
    plt.figure(figsize=(14, 15))
    for i, idx in enumerate(random_indices):
        plt.subplot(4, 3, i+1)
        plt.imshow(Data_test[idx])
        if predictions[i] == np.argmax(Classes_test[idx]):
            title_color = 'g'  # green color for correct predictions
        else:
            title_color = 'r'  # red color for incorrect predictions
        plt.title(class_names[np.argmax(Classes_test[idx])], color=title_color)
        plt.axis('off')

    plt.savefig('prediction_samples.png')
    plt.close()

plot_prediction(Data_test, Classes_test, n_images=6, class_names=class_names)

model.save("GenderClassification.h5")

# ============================================================================
# CCTV Footage Analysis GUI Application
# ============================================================================

class CCTVGenderAnalyzer:
    def __init__(self, root):
        self.root = root
        self.root.title("Live CCTV Gender Classification Analyzer")
        self.root.geometry("1000x700")
        self.root.configure(bg='#f0f0f0')
        
        self.model = None
        self.face_cascade = None
        self.camera = None
        self.is_running = False
        self.tracking_thread = None
        
        # Person tracking variables
        self.tracked_people = []  # List of dictionaries with face info and gender
        self.face_tracking_buffer = []  # Recent face positions for tracking
        self.male_count = 0
        self.female_count = 0
        self.total_people_counted = 0
        
        # Tracking parameters
        self.tracking_threshold = 50  # Distance threshold for same person
        self.tracking_frames = 10  # Frames to remember face positions
        
        # Try to load saved model
        self.load_model()
        self.load_face_cascade()
        
        self.setup_ui()
        
    def load_model(self):
        """Load the trained gender classification model"""
        try:
            if os.path.exists("GenderClassification.h5"):
                print("Loading saved model...")
                self.model = load_model("GenderClassification.h5")
                print("Model loaded successfully!")
            else:
                print("No saved model found. Checking for model in memory...")
                # Check if model exists in global scope (from training)
                if 'model' in globals() and model is not None:
                    print("Using the trained model from memory.")
                    self.model = model
                else:
                    print("WARNING: No model available. Please train the model first.")
                    self.model = None
        except Exception as e:
            print(f"Error loading model: {e}")
            import traceback
            traceback.print_exc()
            # Try to use model from memory as fallback
            if 'model' in globals() and model is not None:
                print("Using model from memory as fallback.")
                self.model = model
            else:
                self.model = None
    
    def load_face_cascade(self):
        """Load OpenCV face detection cascade"""
        try:
            # Try to load the cascade file
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            if self.face_cascade.empty():
                raise Exception("Could not load face cascade")
        except Exception as e:
            print(f"Error loading face cascade: {e}")
            messagebox.showerror("Error", "Could not load face detection model. Please check OpenCV installation.")
    
    def setup_ui(self):
        """Setup the user interface"""
        # Title
        title_label = tk.Label(self.root, text="Live CCTV Gender Classification", 
                              font=("Arial", 20, "bold"), bg='#f0f0f0')
        title_label.pack(pady=15)
        
        # Instructions
        instruction_label = tk.Label(self.root, 
                                    text="Click 'Start Camera' to begin live gender classification",
                                    font=("Arial", 12), bg='#f0f0f0')
        instruction_label.pack(pady=5)
        
        # Control buttons frame
        control_frame = tk.Frame(self.root, bg='#f0f0f0')
        control_frame.pack(pady=15)
        
        # Start/Stop button
        self.start_stop_btn = tk.Button(control_frame, text="Start Camera", 
                                        command=self.toggle_camera, font=("Arial", 12),
                                        bg='#4CAF50', fg='white', padx=20, pady=10,
                                        cursor='hand2')
        self.start_stop_btn.pack(side=tk.LEFT, padx=10)
        
        # Reset button
        reset_btn = tk.Button(control_frame, text="Reset Count", 
                              command=self.reset_count, font=("Arial", 12),
                              bg='#FF9800', fg='white', padx=20, pady=10,
                              cursor='hand2')
        reset_btn.pack(side=tk.LEFT, padx=10)
        
        # Statistics frame
        stats_frame = tk.Frame(self.root, bg='#f0f0f0')
        stats_frame.pack(pady=10, padx=20, fill='x')
        
        # Statistics labels
        self.male_label = tk.Label(stats_frame, text="Male: 0", 
                                   font=("Arial", 14, "bold"), bg='#f0f0f0', fg='#2196F3')
        self.male_label.pack(side=tk.LEFT, padx=20)
        
        self.female_label = tk.Label(stats_frame, text="Female: 0", 
                                     font=("Arial", 14, "bold"), bg='#f0f0f0', fg='#E91E63')
        self.female_label.pack(side=tk.LEFT, padx=20)
        
        self.total_label = tk.Label(stats_frame, text="Total: 0", 
                                    font=("Arial", 14, "bold"), bg='#f0f0f0')
        self.total_label.pack(side=tk.LEFT, padx=20)
        
        # Status label
        self.status_label = tk.Label(self.root, text="Camera: Off", 
                                     font=("Arial", 10), bg='#f0f0f0', fg='red')
        self.status_label.pack(pady=5)
        
        # Results frame
        results_frame = tk.Frame(self.root, bg='#f0f0f0')
        results_frame.pack(pady=10, padx=20, fill='both', expand=True)
        
        # Results title
        results_title = tk.Label(results_frame, text="Live Detection Log", 
                                font=("Arial", 12, "bold"), bg='#f0f0f0')
        results_title.pack(pady=5)
        
        # Results display
        self.results_text = tk.Text(results_frame, height=8, width=80, 
                                    font=("Arial", 9), wrap=tk.WORD,
                                    bg='white', relief=tk.SUNKEN, borderwidth=2)
        self.results_text.pack(pady=5, fill='both', expand=True)
        
        scrollbar = tk.Scrollbar(self.results_text)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.results_text.config(yscrollcommand=scrollbar.set)
        scrollbar.config(command=self.results_text.yview)
        
        # Add initial message
        self.results_text.insert(tk.END, "Ready to start live camera feed...\n")
        self.results_text.insert(tk.END, "Click 'Start Camera' to begin detection.\n\n")
    
    def toggle_camera(self):
        """Start or stop the live camera feed"""
        if not self.is_running:
            self.start_camera()
        else:
            self.stop_camera()
    
    def start_camera(self):
        """Start live camera feed"""
        if not self.model:
            messagebox.showerror("Error", "Model not available. Please train the model first.\n\nMake sure 'GenderClassification.h5' exists or run the training code.")
            return
        
        if not self.face_cascade or self.face_cascade.empty():
            messagebox.showerror("Error", "Face detection model not available.\n\nPlease check OpenCV installation.")
            return
        
        try:
            self.camera = cv2.VideoCapture(0)
            if not self.camera.isOpened():
                raise Exception("Could not open camera")
            
            self.is_running = True
            self.start_stop_btn.config(text="Stop Camera", bg='#f44336')
            self.status_label.config(text="Camera: On", fg='green')
            self.results_text.delete(1.0, tk.END)
            self.results_text.insert(tk.END, "Camera started. Detecting faces...\n\n")
            
            # Start tracking thread
            self.tracking_thread = threading.Thread(target=self._process_camera_feed, daemon=True)
            self.tracking_thread.start()
            
        except Exception as e:
            messagebox.showerror("Error", f"Could not start camera: {str(e)}")
            if self.camera:
                self.camera.release()
                self.camera = None
    
    def stop_camera(self):
        """Stop live camera feed"""
        self.is_running = False
        if self.camera:
            self.camera.release()
            self.camera = None
        
        self.start_stop_btn.config(text="Start Camera", bg='#4CAF50')
        self.status_label.config(text="Camera: Off", fg='red')
        self.results_text.insert(tk.END, "\nCamera stopped.\n")
    
    def reset_count(self):
        """Reset the person count"""
        self.male_count = 0
        self.female_count = 0
        self.total_people_counted = 0
        self.tracked_people = []
        self.face_tracking_buffer = []
        self.update_statistics()
        self.results_text.delete(1.0, tk.END)
        self.results_text.insert(tk.END, "Count reset. Ready for new detections...\n\n")
    
    def update_statistics(self):
        """Update the statistics labels"""
        self.male_label.config(text=f"Male: {self.male_count}")
        self.female_label.config(text=f"Female: {self.female_count}")
        self.total_label.config(text=f"Total: {self.total_people_counted}")
    
    def detect_faces(self, frame):
        """Detect faces in a frame"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.1,
            minNeighbors=5,
            minSize=(30, 30)
        )
        return faces
    
    def calculate_face_center(self, x, y, w, h):
        """Calculate the center point of a face"""
        return (x + w // 2, y + h // 2)
    
    def calculate_distance(self, point1, point2):
        """Calculate Euclidean distance between two points"""
        return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)
    
    def is_new_person(self, face_center):
        """Check if a face belongs to a new person"""
        # Check against recent tracking buffer
        for tracked_face in self.face_tracking_buffer:
            distance = self.calculate_distance(face_center, tracked_face['center'])
            if distance < self.tracking_threshold:
                return False, tracked_face.get('gender', None)
        
        # Check against all tracked people
        for person in self.tracked_people:
            distance = self.calculate_distance(face_center, person['center'])
            if distance < self.tracking_threshold:
                return False, person.get('gender', None)
        
        return True, None
    
    def preprocess_face(self, face_roi):
        """Preprocess face ROI for gender classification"""
        # Resize to match model input size (200, 100) = (height, width)
        # cv2.resize takes (width, height), so we use (100, 200)
        face_resized = cv2.resize(face_roi, (100, 200))
        # Convert BGR to RGB
        face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)
        # Preprocess for ResNet50
        face_array = preprocess_input(face_rgb)
        # Expand dimensions for batch
        face_array = np.expand_dims(face_array, axis=0)
        return face_array
    
    def _process_camera_feed(self):
        """Process live camera feed in background thread"""
        window_name = "Live CCTV - Gender Classification"
        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(window_name, 800, 600)
        
        frame_count = 0
        
        try:
            while self.is_running and self.camera and self.camera.isOpened():
                ret, frame = self.camera.read()
                if not ret:
                    break
                
                # Flip frame horizontally for mirror effect
                frame = cv2.flip(frame, 1)
                
                # Detect faces
                faces = self.detect_faces(frame)
                
                # Update tracking buffer (keep only recent frames)
                if len(self.face_tracking_buffer) > self.tracking_frames:
                    self.face_tracking_buffer.pop(0)
                
                current_frame_faces = []
                
                for (x, y, w, h) in faces:
                    # Extract face ROI
                    face_roi = frame[y:y+h, x:x+w]
                    
                    # Skip if face ROI is too small
                    if face_roi.size == 0 or w < 30 or h < 30:
                        continue
                    
                    # Calculate face center
                    face_center = self.calculate_face_center(x, y, w, h)
                    
                    # Check if this is a new person
                    is_new, existing_gender = self.is_new_person(face_center)
                    
                    try:
                        # Preprocess and predict
                        face_array = self.preprocess_face(face_roi)
                        prediction = self.model.predict(face_array, verbose=0)
                        gender_idx = np.argmax(prediction[0])
                        confidence = prediction[0][gender_idx]
                        
                        gender = "MALE" if gender_idx == 0 else "FEMALE"
                        
                        # Add to current frame tracking
                        current_frame_faces.append({
                            'center': face_center,
                            'gender': gender,
                            'bbox': (x, y, w, h),
                            'confidence': confidence
                        })
                        
                        # If new person, count them
                        if is_new:
                            self.total_people_counted += 1
                            
                            if gender_idx == 0:  # MALE
                                self.male_count += 1
                            else:  # FEMALE
                                self.female_count += 1
                            
                            # Add to tracked people
                            self.tracked_people.append({
                                'center': face_center,
                                'gender': gender,
                                'bbox': (x, y, w, h),
                                'confidence': confidence,
                                'first_seen': frame_count
                            })
                            
                            # Update UI
                            if self.root.winfo_exists():
                                log_msg = f"[{frame_count}] New {gender} detected! (Confidence: {confidence:.2f})\n"
                                self.root.after(0, self._update_results, log_msg)
                                self.root.after(0, self.update_statistics)
                        
                        # Draw bounding box and label
                        color = (255, 0, 0) if gender_idx == 0 else (255, 0, 255)  # Blue for male, Magenta for female
                        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
                        
                        # Label with gender and confidence
                        label = f"{gender} ({confidence:.2f})"
                        if is_new:
                            label += " [NEW]"
                        
                        # Put text on frame
                        cv2.putText(frame, label, (x, y-10), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                        
                    except Exception as e:
                        print(f"Error processing face: {e}")
                        continue
                
                # Update tracking buffer with current frame faces
                self.face_tracking_buffer.extend(current_frame_faces)
                
                # Draw statistics on frame
                stats_text = [
                    f"Male: {self.male_count}",
                    f"Female: {self.female_count}",
                    f"Total: {self.total_people_counted}"
                ]
                
                y_offset = 30
                for i, text in enumerate(stats_text):
                    cv2.putText(frame, text, (10, y_offset + i*25), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                # Show frame
                cv2.imshow(window_name, frame)
                
                # Break on 'q' key press
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
                
                frame_count += 1
                
                # Small delay to prevent overwhelming the system
                import time
                time.sleep(0.03)  # ~30 FPS
                
        except Exception as e:
            import traceback
            error_msg = f"Error in camera feed: {str(e)}"
            print(f"Error in camera feed thread: {error_msg}")
            print(traceback.format_exc())
            if self.root.winfo_exists():
                self.root.after(0, self._show_error, error_msg)
        finally:
            cv2.destroyAllWindows()
            if self.camera:
                self.camera.release()
            self.is_running = False
            if self.root.winfo_exists():
                self.root.after(0, self.stop_camera)
    
    def _update_results(self, text):
        """Update results text (thread-safe)"""
        try:
            if hasattr(self, 'results_text') and self.results_text.winfo_exists():
                self.results_text.insert(tk.END, text)
                self.results_text.see(tk.END)
        except (tk.TclError, AttributeError, RuntimeError):
            # GUI was closed or object doesn't exist, ignore
            pass
    
    def _show_error(self, error_msg):
        """Show error message (thread-safe)"""
        try:
            if not self.root.winfo_exists():
                return
            
            self.results_text.insert(tk.END, f"\nERROR: {error_msg}\n")
            
            # Use after to ensure messagebox is shown after GUI update
            def show_error_popup():
                try:
                    if self.root.winfo_exists():
                        messagebox.showerror("Error", error_msg)
                except:
                    pass
            
            self.root.after(100, show_error_popup)
        except (tk.TclError, AttributeError, RuntimeError):
            # GUI was closed, ignore
            pass

# Run the GUI application
if __name__ == "__main__":
    print("\n" + "="*60)
    print("Training completed! Starting CCTV Gender Classification Analyzer GUI...")
    print("="*60 + "\n")
    
    # Ensure matplotlib backend is set before GUI starts
    matplotlib.use('TkAgg')  # Switch to TkAgg for GUI compatibility
    
    try:
        root = tk.Tk()
        app = CCTVGenderAnalyzer(root)
        
        def on_closing():
            """Handle window closing properly"""
            try:
                # Stop camera if running
                if app.is_running:
                    app.stop_camera()
                # Close OpenCV windows
                cv2.destroyAllWindows()
                root.quit()
                root.destroy()
            except:
                pass
        
        root.protocol("WM_DELETE_WINDOW", on_closing)
        root.mainloop()
    except KeyboardInterrupt:
        print("\nGUI closed by user.")
    except Exception as e:
        print(f"Error starting GUI: {e}")
        import traceback
        traceback.print_exc()

"""# Task
Adapt the existing notebook to use the "CCTV Gender Classifier Dataset" from Google Drive, located at `/content/drive/MyDrive/ML Datamorphosis/CCTV Gender Classifier Dataset.zip`, by unzipping it to a local directory and updating the data paths in the notebook to reflect this change.

## Remove Kaggle Download

### Subtask:
Remove the `kagglehub.dataset_download` code from the initial cell as we will be using data from Google Drive instead.

**Reasoning**:
The subtask is to remove the kagglehub.dataset_download line. I will modify the first code cell to remove this line, as data will be sourced from Google Drive.
"""

# Dataset paths have been updated to use local Windows directories